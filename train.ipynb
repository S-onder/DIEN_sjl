{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/suibe/dev_sjl/毕业论文/data/'\n",
    "ad_features = pd.read_csv(data_path + 'ad_feature.csv')\n",
    "user_profile = pd.read_csv(data_path + 'user_profile.csv')\n",
    "user_profile.rename(columns={'userid' : 'user_id'}, inplace=True)\n",
    "user_profile.rename(columns={'new_user_class_level ': 'new_user_class_level'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(data_path + 'sample.csv')\n",
    "sample_behavior_log = pd.read_csv(data_path + 'sample_behavior_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3244427/1606786277.py:2: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "  sample_behavior_log['unix_time'] = sample_behavior_log['time_stamp'].astype('int64') // 10**9\n"
     ]
    }
   ],
   "source": [
    "sample_behavior_log['time_stamp'] = pd.to_datetime(sample_behavior_log['time_stamp'])\n",
    "sample_behavior_log['unix_time'] = sample_behavior_log['time_stamp'].astype('int64') // 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>btag</th>\n",
       "      <th>cate</th>\n",
       "      <th>brand</th>\n",
       "      <th>unix_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-06 00:49:31</td>\n",
       "      <td>3</td>\n",
       "      <td>4284</td>\n",
       "      <td>41299</td>\n",
       "      <td>1494031771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-05-06 00:46:27</td>\n",
       "      <td>3</td>\n",
       "      <td>4284</td>\n",
       "      <td>342498</td>\n",
       "      <td>1494031587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-04-30 09:48:11</td>\n",
       "      <td>3</td>\n",
       "      <td>6511</td>\n",
       "      <td>374258</td>\n",
       "      <td>1493545691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-04-30 09:48:27</td>\n",
       "      <td>3</td>\n",
       "      <td>6511</td>\n",
       "      <td>374258</td>\n",
       "      <td>1493545707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2017-04-30 09:48:29</td>\n",
       "      <td>3</td>\n",
       "      <td>6511</td>\n",
       "      <td>374258</td>\n",
       "      <td>1493545709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512014</th>\n",
       "      <td>1141718</td>\n",
       "      <td>2017-05-03 16:57:57</td>\n",
       "      <td>3</td>\n",
       "      <td>4282</td>\n",
       "      <td>106054</td>\n",
       "      <td>1493830677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512015</th>\n",
       "      <td>1141718</td>\n",
       "      <td>2017-05-03 16:49:03</td>\n",
       "      <td>3</td>\n",
       "      <td>4520</td>\n",
       "      <td>143597</td>\n",
       "      <td>1493830143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512016</th>\n",
       "      <td>1141718</td>\n",
       "      <td>2017-05-04 06:25:00</td>\n",
       "      <td>3</td>\n",
       "      <td>6300</td>\n",
       "      <td>143597</td>\n",
       "      <td>1493879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512017</th>\n",
       "      <td>1141718</td>\n",
       "      <td>2017-05-03 16:44:37</td>\n",
       "      <td>3</td>\n",
       "      <td>6427</td>\n",
       "      <td>3014</td>\n",
       "      <td>1493829877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512018</th>\n",
       "      <td>1141718</td>\n",
       "      <td>2017-05-04 06:23:54</td>\n",
       "      <td>3</td>\n",
       "      <td>5467</td>\n",
       "      <td>440306</td>\n",
       "      <td>1493879034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7512019 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user          time_stamp  btag  cate   brand   unix_time\n",
       "0              3 2017-05-06 00:49:31     3  4284   41299  1494031771\n",
       "1              3 2017-05-06 00:46:27     3  4284  342498  1494031587\n",
       "2              3 2017-04-30 09:48:11     3  6511  374258  1493545691\n",
       "3              3 2017-04-30 09:48:27     3  6511  374258  1493545707\n",
       "4              3 2017-04-30 09:48:29     3  6511  374258  1493545709\n",
       "...          ...                 ...   ...   ...     ...         ...\n",
       "7512014  1141718 2017-05-03 16:57:57     3  4282  106054  1493830677\n",
       "7512015  1141718 2017-05-03 16:49:03     3  4520  143597  1493830143\n",
       "7512016  1141718 2017-05-04 06:25:00     3  6300  143597  1493879100\n",
       "7512017  1141718 2017-05-03 16:44:37     3  6427    3014  1493829877\n",
       "7512018  1141718 2017-05-04 06:23:54     3  5467  440306  1493879034\n",
       "\n",
       "[7512019 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对btag进行LabelEncoder编码\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "sample_behavior_log['btag'] = le.fit_transform(sample_behavior_log['btag'])\n",
    "sample_behavior_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将时间戳归一化\n",
    "sample_behavior_log['standard_time'] = (sample_behavior_log['unix_time'] - sample_behavior_log['unix_time'].mean()) / sample_behavior_log['unix_time'].std()\n",
    "# price进行归一化\n",
    "# ad_features['price'] = (ad_features['price'] - ad_features['price'].mean()) / ad_features['price'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有的序列取前100个\n",
    "top100_behavior = sample_behavior_log.sort_values(by=['user', 'time_stamp'], ascending=[True, False]).groupby('user').head(100)\n",
    "#  btag序列\n",
    "btag_hist = top100_behavior.groupby('user')['btag'].apply(list).reset_index().rename(columns={'btag': 'btag_hist','user':'user_id'})\n",
    "# cate序列\n",
    "cate_hist = top100_behavior.groupby('user')['cate'].apply(list).reset_index().rename(columns={'cate': 'cate_hist','user':'user_id'})\n",
    "# brand序列\n",
    "brand_hist = top100_behavior.groupby('user')['brand'].apply(list).reset_index().rename(columns={'brand': 'brand_hist','user':'user_id'})\n",
    "# 时间戳序列\n",
    "time_hist = top100_behavior.groupby('user')['standard_time'].apply(list).reset_index().rename(columns={'standard_time': 'time_hist','user':'user_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.merge(sample, ad_features, on = 'adgroup_id', how = 'left')\n",
    "sample = pd.merge(sample, user_profile, on = 'user_id', how = 'left')\n",
    "sample = pd.merge(sample, btag_hist, on = 'user_id', how = 'left')\n",
    "sample = pd.merge(sample, cate_hist, on = 'user_id', how = 'left')\n",
    "sample = pd.merge(sample, brand_hist, on = 'user_id', how = 'left')\n",
    "sample = pd.merge(sample, time_hist, on = 'user_id', how = 'left')\n",
    "sample.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(Dataset):\n",
    "    def __init__(self, df, max_seq_len, user_feature_cols, ad_feature_cols, label_col):\n",
    "        \"\"\"\n",
    "        :param df: 原始 DataFrame\n",
    "        :param max_seq_len: 最大序列长度，用于 padding 序列\n",
    "        :param user_feature_cols: 用户特征列名列表\n",
    "        :param ad_feature_cols: 广告特征列名列表\n",
    "        # :param seq_feature_col: 序列特征列名\n",
    "        :param label_col: 标签列名\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.user_feature_cols = user_feature_cols\n",
    "        self.ad_feature_cols = ad_feature_cols\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 用户特征\n",
    "        user_features = torch.tensor(row[self.user_feature_cols], dtype=torch.float)\n",
    "        \n",
    "        # 广告特征\n",
    "        ad_features = torch.tensor(row[self.ad_feature_cols], dtype=torch.float)\n",
    "        \n",
    "        # 序列特征 (需要 padding)\n",
    "        btag_hist = self.pad_sequence(row['btag_hist'], self.max_seq_len, padding_value=0, dtype=torch.int64)\n",
    "        cate_hist = self.pad_sequence(row['cate_hist'], self.max_seq_len, padding_value=0, dtype=torch.int64)\n",
    "        brand_hist = self.pad_sequence(row['brand_hist'], self.max_seq_len, padding_value=0, dtype=torch.int64)\n",
    "        time_hist = self.pad_sequence(row['time_hist'], self.max_seq_len, padding_value=0.0, dtype=torch.int64)\n",
    "        \n",
    "        # 标签\n",
    "        label = torch.tensor(row[self.label_col], dtype=torch.float)\n",
    "        #\n",
    "        \n",
    "        return user_features, ad_features, (btag_hist, cate_hist,brand_hist,time_hist), label\n",
    "\n",
    "    def pad_sequence(self, sequence, max_len, padding_value, dtype):\n",
    "        \"\"\"\n",
    "        对输入序列进行padding。\n",
    "        - sequence: 输入的序列 (list)\n",
    "        - max_len: 需要padding的最大长度\n",
    "        - padding_value: 用于填充的值 (默认: 0)\n",
    "        - dtype: 转换后的数据类型 (默认: torch.int64)\n",
    "        \"\"\"\n",
    "        seq_len = len(sequence)\n",
    "        # 如果序列长度超过 max_len，进行截断\n",
    "        if seq_len > max_len:\n",
    "            sequence = sequence[:max_len]\n",
    "        else:\n",
    "            # 否则进行 padding\n",
    "            sequence = sequence + [padding_value] * (max_len - seq_len)\n",
    "\n",
    "        # 转换为Tensor\n",
    "        padded_seq = torch.tensor(sequence, dtype=dtype)\n",
    "        return padded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户特征列\n",
    "user_feature_cols = ['user_id','cms_segid', 'cms_group_id' ,'final_gender_code',\n",
    "                      'age_level', 'pvalue_level', 'shopping_level', 'occupation', 'new_user_class_level']\n",
    "# 广告特征列\n",
    "ad_feature_cols = ['adgroup_id', 'cate_id', 'campaign_id', 'brand', 'customer']\n",
    "# 标签列\n",
    "label_col = 'clk'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100  # 假设最大序列长度为100\n",
    "dataset = mydata(sample, max_seq_len, user_feature_cols, ad_feature_cols, label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 DataLoader\n",
    "batch_size = 256\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    user_features, ad_features, (btag_hist, cate_hist,brand_hist,time_hist), label = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建DeppFM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_dim = sample.user_id.max() + 1\n",
    "ad_feature_dim = sample.adgroup_id.max() + 1\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "user_embedding = nn.Embedding(user_feature_dim, embedding_dim)\n",
    "ad_embedding = nn.Embedding(ad_feature_dim, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embed = user_embedding(user_features.long())\n",
    "ad_embed = ad_embedding(ad_features.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别计算用户特征和广告特征的二阶交互项\n",
    "user_sum_square_embed = torch.pow(user_embed.sum(dim=1), 2)\n",
    "user_square_sum_embed = torch.pow(user_embed, 2).sum(dim=1)\n",
    "fm_user_interaction = 0.5 * (user_sum_square_embed - user_square_sum_embed)\n",
    "\n",
    "ad_sum_square_embed = torch.pow(ad_embed.sum(dim=1), 2)\n",
    "ad_square_sum_embed = torch.pow(ad_embed, 2).sum(dim=1)\n",
    "fm_ad_interaction = 0.5 * (ad_sum_square_embed - ad_square_sum_embed)\n",
    "\n",
    "# 将用户和广告的交互项相加或拼接\n",
    "fm_interaction = fm_user_interaction + fm_ad_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class DeepFM(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepF模型 Pytorch版本\n",
    "    \"\"\"\n",
    "    def __init__(self, user_feature_dim, ad_feature_dim, embedding_dim, hidden_dims, dropout_rate):\n",
    "        \"\"\"\n",
    "        :param feature_dim: 特征维度(所有特征共享embedding)\n",
    "        :param embedding_dim: embedding维度\n",
    "        :param hidden_dims: 隐藏层维度列表\n",
    "        :param dropout_rate: dropout比率\n",
    "        \"\"\"\n",
    "        super(DeepFM, self).__init__()\n",
    "        # 分别为用户特征和广告特征创建独立的 embedding 层\n",
    "        self.user_embedding = nn.Embedding(user_feature_dim, embedding_dim)\n",
    "        self.ad_embedding = nn.Embedding(ad_feature_dim, embedding_dim)\n",
    "\n",
    "        # FM部分（交叉项）\n",
    "        # 一阶特征 embedding\n",
    "        self.fm_first_order_user = nn.Embedding(user_feature_dim, 1)\n",
    "        self.fm_first_order_ad = nn.Embedding(ad_feature_dim, 1)\n",
    "\n",
    "        # Deep部分\n",
    "        all_dims = [2 * embedding_dim] + hidden_dims # [512, 256, 128]\n",
    "        layers = []\n",
    "        for i in range(len(all_dims)-1):\n",
    "            layers.append(nn.Linear(all_dims[i], all_dims[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        self.dnn = nn.Sequential(*layers)\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1] + embedding_dim + 2, 1) #DNN+FFM\n",
    "\n",
    "    def forward(self, user_features, ad_features):\n",
    "        \"\"\"\n",
    "        :param user_features: 用户特征\n",
    "        :param ad_features: 广告特征\n",
    "        :param behavior_features: 行为特征\n",
    "        \"\"\"\n",
    "        user_features = user_features.long()\n",
    "        ad_features = ad_features.long()\n",
    "        # 嵌入用户特征和广告特征\n",
    "        user_embed = self.user_embedding(user_features)  # 用户特征的 embedding\n",
    "        ad_embed = self.ad_embedding(ad_features)        # 广告特征的 embedding\n",
    "        # FM 一阶项（线性部分）\n",
    "        fm_first_order_user = self.fm_first_order_user(user_features)\n",
    "        fm_first_order_ad = self.fm_first_order_ad(ad_features)\n",
    "       # 拼接用户和广告特征的一阶项\n",
    "        fm_first_order = torch.cat([fm_first_order_user, fm_first_order_ad], dim=1)\n",
    "        # FM 二阶交互项\n",
    "        # 分别计算用户特征和广告特征的二阶交互项\n",
    "        user_sum_square_embed = torch.pow(user_embed.sum(dim=1), 2)\n",
    "        user_square_sum_embed = torch.pow(user_embed, 2).sum(dim=1)\n",
    "        fm_user_interaction = 0.5 * (user_sum_square_embed - user_square_sum_embed)\n",
    "\n",
    "        ad_sum_square_embed = torch.pow(ad_embed.sum(dim=1), 2)\n",
    "        ad_square_sum_embed = torch.pow(ad_embed, 2).sum(dim=1)\n",
    "        fm_ad_interaction = 0.5 * (ad_sum_square_embed - ad_square_sum_embed)\n",
    "\n",
    "        # 将用户和广告的交互项相加或拼接\n",
    "        fm_interaction = fm_user_interaction + fm_ad_interaction\n",
    "        # 将用户和广告的嵌入向量拼接后输入到 DNN 部分\n",
    "        dnn_input = torch.cat([user_embed, ad_embed], dim=-1)\n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "        # 将 DNN 输出、FM 一阶项和 FM 二阶交互项拼接\n",
    "        feature_output = torch.cat([dnn_output, fm_first_order.view(fm_first_order.shape[0], -1), fm_interaction], dim=-1) # 128 + 2 + 256\n",
    "            # 最终输出\n",
    "        output = self.output_layer(feature_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0; 44.34 GiB total capacity; 0 bytes already allocated; 530.69 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev_sjl/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev_sjl/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev_sjl/lib/python3.8/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/dev_sjl/lib/python3.8/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0; 44.34 GiB total capacity; 0 bytes already allocated; 530.69 MiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 参数定义\n",
    "user_feature_dim = sample.user_id.max() + 1\n",
    "ad_feature_dim = sample.adgroup_id.max() + 1\n",
    "embedding_dim = 256\n",
    "dnn_hidden_units = [256, 128]\n",
    "learning_rate = 0.001\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "# 模型实例化\n",
    "model = DeepFM(user_feature_dim, ad_feature_dim, embedding_dim, dnn_hidden_units,dropout_rate=0.1)\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 损失函数\n",
    "criterion = torch.nn.BCELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# 模型训练\n",
    "for epoch in range(epochs):\n",
    "        model.train()  # 切换到训练模式\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # 获取数据，并移动到设备\n",
    "            user_features, ad_features, (btag_hist, cate_hist, brand_hist, time_hist), label = batch\n",
    "            user_features, ad_features, label = user_features.to(device), ad_features.to(device), label.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            output = model(user_features, ad_features)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 累积损失\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 收集输出和标签用于计算AUC\n",
    "            all_outputs.extend(output.detach().cpu().numpy())\n",
    "            all_labels.extend(label.detach().cpu().numpy())\n",
    "\n",
    "            # 每500个step输出一次损失和AUC\n",
    "            if (step + 1) % 500 == 0:\n",
    "                avg_loss = running_loss / 500\n",
    "                auc = roc_auc_score(all_labels, all_outputs)\n",
    "\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{step+1}], Loss: {avg_loss:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "                # 重置累积变量\n",
    "                running_loss = 0.0\n",
    "                all_labels = []\n",
    "                all_outputs = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_sjl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
